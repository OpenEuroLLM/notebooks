{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c8456e-a7b8-487f-a49e-e00fea2d9d06",
   "metadata": {},
   "source": [
    "# Battle annotation\n",
    "\n",
    "This notebook allows to annotate battle preference data from LMSys and ComparIA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d74b72a-ff97-4061-8d3c-a68bdc82cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fast_langdetect import detect_language\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from huggingface_hub import snapshot_download, hf_hub_download, upload_file, list_repo_files\n",
    "from IPython.display import Markdown as md\n",
    "from ipywidgets import IntSlider, RadioButtons, VBox, Output\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af71ljj62pl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Current user: salinasd\n"
     ]
    }
   ],
   "source": [
    "# Configuration for HuggingFace storage\n",
    "HF_REPO = \"openeurollm/battle-annotations\"  # TODO: Update this with your HF repo\n",
    "username = os.getenv(\"USER\")\n",
    "print(f\"üë§ Current user: {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2780c4-779b-4560-b3e4-661f704a99bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6776483e923404ea281407a2f170a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a3c81b5bdb47e4b34c9a12bdfd0489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5282f1de0a74f22b4a3442432fe5cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da9719e1b174405883884d3068d8e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# we fix the version as comparia is continuously increasing\n",
    "def load_df(\n",
    "    comparia_revision: str = \"538ead8c4dad4ff905cd2f11d7381d7df03d3fdc\",\n",
    ") -> pd.DataFrame:\n",
    "    # load LMSys\n",
    "    path = snapshot_download(\n",
    "        repo_id=\"lmarena-ai/arena-human-preference-100k\",\n",
    "        repo_type=\"dataset\",\n",
    "        allow_patterns=\"*parquet\",\n",
    "        force_download=False,\n",
    "    )\n",
    "    df_lmsys = pd.read_parquet(\n",
    "        Path(path) / \"data\" / \"arena-explorer-preference-100k.parquet\"\n",
    "    )\n",
    "    df_lmsys[\"date\"] = pd.to_datetime(df_lmsys[\"tstamp\"], unit=\"s\")\n",
    "    df_lmsys[\"benchmark\"] = \"LMSys\"\n",
    "\n",
    "    # load ComparIA\n",
    "    path = snapshot_download(\n",
    "        repo_id=\"ministere-culture/comparia-votes\",\n",
    "        repo_type=\"dataset\",\n",
    "        allow_patterns=\"*\",\n",
    "        revision=comparia_revision,\n",
    "        force_download=False,\n",
    "    )\n",
    "\n",
    "    df_comparia = pd.read_parquet(Path(path) / \"votes.parquet\")\n",
    "\n",
    "    # unify schema\n",
    "    df_comparia[\"tstamp\"] = df_comparia[\"timestamp\"]\n",
    "    df_comparia[\"model_a\"] = df_comparia[\"model_a_name\"]\n",
    "    df_comparia[\"model_b\"] = df_comparia[\"model_b_name\"]\n",
    "\n",
    "    def get_winner(\n",
    "        chosen_model_name: str, model_a: str, model_b: str, both_equal: bool, **kwargs\n",
    "    ):\n",
    "        if both_equal or chosen_model_name is None:\n",
    "            return \"tie\"\n",
    "        else:\n",
    "            assert chosen_model_name in [\n",
    "                model_a,\n",
    "                model_b,\n",
    "            ], f\"Chosen model: {chosen_model_name} but model_a: {model_a} and model_b: {model_b}\"\n",
    "            return \"model_a\" if chosen_model_name == model_a else \"model_b\"\n",
    "\n",
    "    df_comparia[\"winner\"] = df_comparia.apply(lambda row: get_winner(**row), axis=1)\n",
    "    df_comparia[\"benchmark\"] = \"ComparIA\"\n",
    "    df_comparia[\"question_id\"] = df_comparia[\"id\"]\n",
    "    cols = [\n",
    "        \"question_id\",\n",
    "        \"tstamp\",\n",
    "        \"model_a\",\n",
    "        \"model_b\",\n",
    "        \"winner\",\n",
    "        \"conversation_a\",\n",
    "        \"conversation_b\",\n",
    "        \"benchmark\",\n",
    "    ]\n",
    "    df = pd.concat([df_lmsys.loc[:, cols], df_comparia.loc[:, cols]], ignore_index=True)\n",
    "\n",
    "    # keep only one turn conversation for now as they are easier to evaluate\n",
    "    df[\"turns\"] = df.apply(lambda row: len(row[\"conversation_a\"]) - 1, axis=1)\n",
    "    df = df.loc[df.turns == 1]\n",
    "\n",
    "    df[\"lang\"] = df.apply(\n",
    "        lambda row: detect_language(row[\"conversation_a\"][0][\"content\"]).lower(), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "    \n",
    "df = load_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de58de2-c9b3-48c1-8bcc-2056643644d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a language to annotate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99e344dbdab4ad195c0039394dd7db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Language:', options=('af', 'am', 'ar', 'arz', 'ast', 'az', 'be', '‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "languages = sorted(df.lang.unique())\n",
    "language = languages[0]\n",
    "\n",
    "print(\"Select a language to annotate\")\n",
    "\n",
    "@interact(lang=Dropdown(options=languages, value=languages[0], description='Language:'))\n",
    "def select_language(lang):\n",
    "    global language\n",
    "    language = lang\n",
    "    print(f\"Selected: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9943d9f8-a9d0-4812-9b40-af06ca9ab99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:27:13.144853Z",
     "start_time": "2025-12-02T17:27:12.640727Z"
    }
   },
   "outputs": [],
   "source": [
    "n_instructions = 100\n",
    "\n",
    "# fix the sampling seed\n",
    "seed = 0\n",
    "\n",
    "df_lmsys = df.loc[(df[\"lang\"] == language) & (df[\"benchmark\"] == \"LMSys\") & (df[\"turns\"] == 1)]\n",
    "df_comparia = df.loc[(df[\"lang\"] == language) & (df[\"benchmark\"] == \"ComparIA\") & (df[\"turns\"] == 1)]\n",
    "\n",
    "# sample n // 2 for each source of battles\n",
    "df_sample = pd.concat([\n",
    "    df_lmsys.sample(\n",
    "        n=n_instructions // 2, random_state=seed\n",
    "    ),\n",
    "    df_comparia.sample(\n",
    "        n=n_instructions // 2, random_state=seed\n",
    "    ),\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad6a702-0ed1-45ea-90d0-92dffa3f7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c605572-878e-4a34-a70c-eae4ab195979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d4995e5f7846979fbd2bbb9ed32448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "salinasd_fr.parquet:   0%|          | 0.00/211k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 67 existing annotations for salinasd/fr\n",
      "üìÖ Latest annotation: 2025-10-12T03:40:14.792200\n",
      "üìä Total annotations loaded: 67\n"
     ]
    }
   ],
   "source": [
    "# Load existing annotations from HuggingFace if they exist\n",
    "# This will preserve your previous work and avoid overriding annotations\n",
    "filename = f\"{username}_{language}.parquet\"\n",
    "\n",
    "annotations = {}\n",
    "try:\n",
    "    files = list_repo_files(HF_REPO, repo_type=\"dataset\")\n",
    "    if filename in files:\n",
    "        local_path = hf_hub_download(HF_REPO, filename, repo_type=\"dataset\")\n",
    "        existing_df = pd.read_parquet(local_path)\n",
    "        # Load annotations with question_id as the key\n",
    "        for _, row in existing_df.iterrows():\n",
    "            annotations[row[\"question_id\"]] = {\n",
    "                \"annotation\": row[\"annotation\"],\n",
    "                \"timestamp\": row[\"timestamp\"],\n",
    "                \"i\": row[\"i\"],\n",
    "                \"question_id\": row[\"question_id\"],\n",
    "                #\"tstamp\": row[\"tstamp\"],\n",
    "                \"model_a\": row[\"model_a\"],\n",
    "                \"model_b\": row[\"model_b\"],\n",
    "                \"winner\": row[\"winner\"],\n",
    "                \"benchmark\": row[\"benchmark\"],\n",
    "                \"conversation_a\": row[\"conversation_a\"],\n",
    "                \"conversation_b\": row[\"conversation_b\"]\n",
    "            }\n",
    "        print(f\"‚úÖ Loaded {len(annotations)} existing annotations for {username}/{language}\")\n",
    "        print(f\"üìÖ Latest annotation: {existing_df['timestamp'].max()}\")\n",
    "    else:\n",
    "        print(f\"üÜï No existing annotations found for {filename}, starting fresh\")\n",
    "except Exception as e:    \n",
    "    print(f\"‚ö†Ô∏è  Could not load from HuggingFace (starting fresh): {e}\")\n",
    "    print(f\"   This is normal if you haven't pushed annotations yet or the repo doesn't exist\")\n",
    "\n",
    "print(f\"üìä Total annotations loaded: {len(annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f9fb6a-b34e-461b-9f2a-a088e70136aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c6e83b22814fd6913ec28b7d1bbae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, continuous_update=False, description='Record:', layout=Layout(width='500px')‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309463e8900c46548933c8e52e1c779e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create output widget for displaying records\n",
    "output = Output()\n",
    "\n",
    "# Create slider with step buttons enabled\n",
    "slider = IntSlider(\n",
    "    min=0, \n",
    "    max=len(df_sample) - 1, \n",
    "    step=1, \n",
    "    value=0, \n",
    "    description='Record:', \n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '500px'},\n",
    "    continuous_update=False  # Only update when you release or use arrow buttons\n",
    ")\n",
    "\n",
    "# Create radio buttons\n",
    "radio = RadioButtons(\n",
    "    options=['Not annotated', 'Model A', 'Tie', 'Model B'],\n",
    "    value='Not annotated',\n",
    "    description='Your choice:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def show_record(i: int):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"üìä **Record {i}**\")    \n",
    "        \n",
    "        # Get question_id for current record\n",
    "        row = df_sample.loc[i]\n",
    "        question_id = row['question_id']\n",
    "\n",
    "        # Show annotation status with timestamp\n",
    "        if question_id in annotations:\n",
    "            current_annotation = annotations[question_id]['annotation']\n",
    "            timestamp = annotations[question_id]['timestamp']\n",
    "            print(f\"‚úÖ Current annotation: {current_annotation} (saved at {timestamp})\")\n",
    "        else:\n",
    "            current_annotation = 'Not annotated'\n",
    "            print(f\"‚ö™ Current annotation: Not annotated\")\n",
    "        \n",
    "        print(f\"üìà Total annotated: {len(annotations)} / {len(df_sample)}\")\n",
    "\n",
    "        conv_a = row[\"conversation_a\"]\n",
    "        conv_b = row[\"conversation_b\"]\n",
    "        \n",
    "        print(f'\\n\\033[1müìù Instruction:\\033[0m\\n{conv_a[0][\"content\"]}')\n",
    "        print(f\"\\n\\033[1müí¨ Completion A:\\033[0m\\n{conv_a[1][\"content\"]}\")\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"\\n\\033[1müí¨ Completion B:\\033[0m\\n{conv_b[1][\"content\"]}\")\n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "        \n",
    "\n",
    "def on_slider_change(change):\n",
    "    i = change['new']\n",
    "    # Get question_id and load the annotation for this record\n",
    "    row = df_sample.loc[i]\n",
    "    question_id = row['question_id']\n",
    "    \n",
    "    if question_id in annotations:\n",
    "        radio.value = annotations[question_id]['annotation']\n",
    "    else:\n",
    "        radio.value = 'Not annotated'\n",
    "    show_record(i)\n",
    "\n",
    "def on_radio_change(change):\n",
    "    i = slider.value\n",
    "    row = df_sample.loc[i]\n",
    "    question_id = row['question_id']\n",
    "    \n",
    "    # Store annotation with timestamp and question details using question_id as key\n",
    "    if change['new'] != 'Not annotated':\n",
    "        annotations[question_id] = {\n",
    "            'annotation': change['new'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'i': i,\n",
    "            'question_id': question_id,\n",
    "            'tstamp': row['tstamp'],\n",
    "            'model_a': row['model_a'],\n",
    "            'model_b': row['model_b'],\n",
    "            'winner': row['winner'],\n",
    "            \"benchmark\": row[\"benchmark\"],\n",
    "            'conversation_a': row['conversation_a'],\n",
    "            'conversation_b': row['conversation_b']\n",
    "        }\n",
    "    elif question_id in annotations:\n",
    "        # If user selects \"Not annotated\", remove the annotation\n",
    "        del annotations[question_id]\n",
    "    show_record(i)\n",
    "\n",
    "# Attach event handlers\n",
    "slider.observe(on_slider_change, names='value')\n",
    "radio.observe(on_radio_change, names='value')\n",
    "\n",
    "# Create layout\n",
    "controls = VBox([slider, radio])\n",
    "\n",
    "# Display everything\n",
    "display(controls)\n",
    "display(output)\n",
    "\n",
    "# Show initial record\n",
    "show_record(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cfb7604-6e92-4a57-b3b2-fce05518cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f20f0c2b58264167a1f4921d2133e65d': {'annotation': 'Model A',\n",
       "  'timestamp': '2025-12-12T16:00:34.304907',\n",
       "  'i': 0,\n",
       "  'question_id': 'f20f0c2b58264167a1f4921d2133e65d',\n",
       "  'tstamp': 1723120202.3584,\n",
       "  'model_a': 'gemini-1.5-pro-exp-0801',\n",
       "  'model_b': 'mistral-large-2407',\n",
       "  'winner': 'model_a',\n",
       "  'benchmark': 'LMSys',\n",
       "  'conversation_a': array([{'content': 'Gary est 2eme, Tom 3eme, Bryan 4eme. Le 5eme double 2 personnes. Guy double 4 personnes. Hugues √©tait 5eme. \\nQuelle place termine Hugues. Et qui gagne la course?', 'num_tokens': 56, 'role': 'user'},\n",
       "         {'content': '* **Hugues √©tait 5√®me et le 5√®me double 2 personnes:** Cela signifie que Hugues d√©passe deux personnes et termine donc √† la **3√®me place**.\\n\\n* **Guy double 4 personnes:**  On ne sait pas √† quelle position Guy a commenc√©, donc on ne peut pas d√©terminer sa position finale ni qui est devant lui.\\n\\n* **Gary est 2√®me, Tom 3√®me, Bryan 4√®me :**  Avec Hugues qui termine 3√®me, cela signifie que Tom et Bryan sont respectivement 4√®me et 5√®me.\\n\\n**Conclusion:**\\n\\n* **Hugues termine 3√®me.**\\n* **On ne peut pas d√©terminer qui gagne la course car on ne sait pas o√π Guy a commenc√©.** \\n\\nIl manque des informations pour savoir qui est 1er.', 'num_tokens': 188, 'role': 'assistant'}],\n",
       "        dtype=object),\n",
       "  'conversation_b': array([{'content': 'Gary est 2eme, Tom 3eme, Bryan 4eme. Le 5eme double 2 personnes. Guy double 4 personnes. Hugues √©tait 5eme. \\nQuelle place termine Hugues. Et qui gagne la course?', 'num_tokens': 56, 'role': 'user'},\n",
       "         {'content': \"Pour r√©soudre ce probl√®me, nous devons analyser les informations donn√©es et d√©terminer les positions finales de chaque participant.\\n\\n1. Gary est 2√®me.\\n2. Tom est 3√®me.\\n3. Bryan est 4√®me.\\n4. Le 5√®me double 2 personnes.\\n5. Guy double 4 personnes.\\n6. Hugues √©tait 5√®me.\\n\\nCommen√ßons par examiner la position de Hugues. Si Hugues √©tait 5√®me et qu'il double 2 personnes, il passe devant Bryan (4√®me) et Tom (3√®me). Cela signifie que Hugues termine 3√®me.\\n\\nMaintenant, regardons la position de Guy. Si Guy double 4 personnes, il passe devant Gary (2√®me), Tom (3√®me), Bryan (4√®me) et Hugues (5√®me). Cela signifie que Guy termine 1er.\\n\\nR√©sumons les positions finales :\\n1. Guy (1er)\\n2. Gary (2√®me)\\n3. Hugues (3√®me)\\n4. Tom (4√®me)\\n5. Bryan (5√®me)\\n\\nDonc, Hugues termine √† la 3√®me place et Guy gagne la course.\", 'num_tokens': 258, 'role': 'assistant'}],\n",
       "        dtype=object)}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3a3bb-228c-4bd6-8048-7d02bf1ca2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c83d28b2-d53a-4645-94f1-8c174e41c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Annotations still missing: {list(set(range(100)).difference(set(annotations_series.index)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b52a4b88-4846-4320-bcc6-82d51d8a4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.DataFrame([\n",
    "    {\n",
    "        \"i\": data['i'],\n",
    "        \"annotation\": data['annotation'], \n",
    "        \"timestamp\": data['timestamp'],\n",
    "        \"question_id\": data['question_id'],\n",
    "        \"tstamp\": data['tstamp'],\n",
    "        \"model_a\": data['model_a'],\n",
    "        \"model_b\": data['model_b'],\n",
    "        \"winner\": data['winner'],\n",
    "        \"benchmark\": data[\"benchmark\"],\n",
    "        \"conversation_a\": data['conversation_a'],\n",
    "        \"conversation_b\": data['conversation_b']\n",
    "    }\n",
    "    for question_id, data in annotations.items()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45fd3c3d-887e-4fef-9ffe-c71c942d48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous local annotations\n",
    "#language = \"en\"\n",
    "#filename = f\"{username}_{language}.parquet\"\n",
    "#annotations_df = pd.read_parquet(f\"/Users/salinasd/Documents/code/openjury/scripts/human_agreement/annotations/battles-annotated-{language}.parquet\")\n",
    "#annotations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f1ef0-4e91-49aa-9eca-4ce25829ca33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa0432a1-9f93-4d3a-808f-5095021bc960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 98 annotations locally\n",
      "annotation\n",
      "Tie              32\n",
      "Model B          27\n",
      "Not annotated    20\n",
      "Model A          19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save annotations locally (backup)\n",
    "# This preserves existing annotations and only updates/adds new ones\n",
    "from pathlib import Path\n",
    "annotation_path = Path(\"annotations\")\n",
    "annotation_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Convert annotations dict to DataFrame with all question details\n",
    "if len(annotations_df) > 0:\n",
    "    \n",
    "    # Save to local files\n",
    "    # annotations_df.to_csv(f\"annotations/annotations-{username}-{language}.csv\", index=False)\n",
    "    annotations_df.to_parquet(f\"annotations/annotations-{username}-{language}.parquet\", index=False)\n",
    "    # df_sample.to_csv(f\"annotations/battles-sampled-{language}.csv\", index=False)\n",
    "    print(f\"‚úÖ Saved {len(annotations_df)} annotations locally\")\n",
    "    print(annotations_df['annotation'].value_counts())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No annotations to save yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eed0wyjlabm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504a75668a6e48d59903367316c6ae00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cbb62318684155ade065a7346a1ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pushed 98 annotations to openeurollm/battle-annotations/salinasd_en.parquet\n",
      "üìÖ Latest timestamp: 2025-10-12T03:40:14.792200\n",
      "\n",
      "Annotation distribution:\n",
      "annotation\n",
      "Tie              32\n",
      "Model B          27\n",
      "Not annotated    20\n",
      "Model A          19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save annotations to HuggingFace\n",
    "# This will upload your annotations to the shared dataset\n",
    "# Run this cell when you want to backup your work to HuggingFace\n",
    "\n",
    "if len(annotations_df) == 0:\n",
    "    print(\"‚ö†Ô∏è  No annotations to save yet\")\n",
    "else:    \n",
    "    # Save and push to HuggingFace\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".parquet\", delete=False) as f:\n",
    "        annotations_df.to_parquet(f.name, index=False)\n",
    "        try:\n",
    "            upload_file(\n",
    "                path_or_fileobj=f.name,\n",
    "                path_in_repo=filename,\n",
    "                repo_id=HF_REPO,\n",
    "                repo_type=\"dataset\",\n",
    "            )\n",
    "            print(f\"‚úÖ Pushed {len(annotations_df)} annotations to {HF_REPO}/{filename}\")\n",
    "            print(f\"üìÖ Latest timestamp: {annotations_df['timestamp'].max()}\")\n",
    "            print(\"\\nAnnotation distribution:\")\n",
    "            print(annotations_df['annotation'].value_counts())\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to push to HuggingFace: {e}\")\n",
    "            print(f\"   Make sure the repo '{HF_REPO}' exists and you have write access\")\n",
    "        finally:\n",
    "            # Clean up temp file\n",
    "            os.unlink(f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947d12c-a877-403e-a2a1-2ea1bf57d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341aecc-24be-43c2-b84a-f142d9e3b20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
